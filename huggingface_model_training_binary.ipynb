{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /Users/marcus/.netrc\n"
     ]
    }
   ],
   "source": [
    "import torch.cuda\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import wandb\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "with open('secrets/wandb_api_key.txt') as f:\n",
    "    wandb.login(key=f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train pre-trained model from hugging face\n",
    "#### Load Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'label', 'text'],\n    num_rows: 315\n})"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.load_from_disk('data/code_search_net_relevance.hf')\n",
    "ds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Convert groups to binary model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def convert_to_binary(data):\n",
    "    if data['label'] in [2, 3]:\n",
    "        data['label'] = 1\n",
    "    return data\n",
    "\n",
    "ds = ds.map(convert_to_binary)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Balance Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'label', 'text'],\n    num_rows: 266\n})"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ds_0 = ds.filter(lambda data: data['label'] == 0)\n",
    "# ds_1 = Dataset.from_dict(ds.filter(lambda data:data['label'] == 1)[:len(ds_0)])\n",
    "# ds = concatenate_datasets([ds_0, ds_1])\n",
    "# ds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load tokenizer and preprocess"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# PRETRAINED_MODEL = 'distilbert-base-uncased'\n",
    "PRETRAINED_MODEL = 'microsoft/codebert-base'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'label', 'text', 'input_ids', 'attention_mask'],\n    num_rows: 266\n})"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "\n",
    "def preprocess(data):\n",
    "    return tokenizer(data['text'], truncation=True, padding=True)\n",
    "\n",
    "data_tokens = ds.map(preprocess)\n",
    "data_tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['label', 'text', 'input_ids', 'attention_mask'],\n    num_rows: 266\n})"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO maybe equalize both classes\n",
    "data_tokens = data_tokens.remove_columns(['func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens'])\n",
    "data_tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "<Axes: xlabel='label', ylabel='count'>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data_tokens.to_pandas(), x='label')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "'<s>Converts the given postgresql seconds to java seconds. Reverse engineered by inserting varying\\ndates to postgresql and tuning the formula until the java dates matched. See {@link #toPgSecs}\\nfor the reverse operation.\\n\\n@param secs Postgresql seconds.\\n@return Java seconds.[SEP]private static long toJavaSecs(long secs) {\\n    // postgres epoc to java epoc\\n    secs += 946684800L;\\n\\n    // Julian/Gregorian calendar cutoff point\\n    if (secs < -12219292800L) { // October 4, 1582 -> October 15, 1582\\n      secs += 86400 * 10;\\n      if (secs < -14825808000L) { // 1500-02-28 -> 1500-03-01\\n        int extraLeaps = (int) ((secs + 14825808000L) / 3155760000L);\\n        extraLeaps--;\\n        extraLeaps -= extraLeaps / 4;\\n        secs += extraLeaps * 86400L;\\n      }\\n    }\\n    return secs;\\n  }</s>'"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data_tokens['input_ids'][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Evaluate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "f1 = evaluate.load('f1')\n",
    "accuracy = evaluate.load('accuracy')\n",
    "recall = evaluate.load('recall')\n",
    "precision = evaluate.load('precision')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy_res = accuracy.compute(predictions=predictions, references=labels)\n",
    "    f1_macro_res = f1.compute(predictions=predictions, references=labels, average='macro')\n",
    "    f1_micro_res = f1.compute(predictions=predictions, references=labels, average='micro')\n",
    "    f1_weighted_res = f1.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    recall_macro_res = recall.compute(predictions=predictions, references=labels, average='macro')\n",
    "    recall_micro_res = recall.compute(predictions=predictions, references=labels, average='micro')\n",
    "    recall_weighted_res = recall.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    precision_macro_res = precision.compute(predictions=predictions, references=labels, average='macro')\n",
    "    precision_micro_res = precision.compute(predictions=predictions, references=labels, average='micro')\n",
    "    precision_weighted_res = precision.compute(predictions=predictions, references=labels, average='weighted')\n",
    "\n",
    "    return {'accuracy': accuracy_res,\n",
    "            'f1_macro': f1_macro_res, 'f1_micro': f1_micro_res, 'f1_weighted': f1_weighted_res,\n",
    "            'recall_macro': recall_macro_res, 'recall_micro': recall_micro_res, 'recall_weighted': recall_weighted_res,\n",
    "            'precision_macro': precision_macro_res, 'precision_micro': precision_micro_res, 'precision_weighted': precision_weighted_res,\n",
    "            }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Split train and test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "train_test_valid = data_tokens.train_test_split(test_size=0.1)\n",
    "test_valid = train_test_valid['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    'train': train_test_valid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']\n",
    "})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "<Axes: xlabel='label', ylabel='count'>"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(datasets['train'].to_pandas(), x='label')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "<Axes: xlabel='label', ylabel='count'>"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(datasets['test'].to_pandas(), x='label')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "<Axes: xlabel='label', ylabel='count'>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(datasets['valid'].to_pandas(), x='label')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "id2label = {0: 'irrelevant', 1: 'relevant'}\n",
    "label2id = {'irrelevant': 0, 'relevant': 1}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/marcus/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111819166625436, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d604d6fcbdde4751ac09b338c94acff2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.12"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/marcus/workspace/Programming/JavaDoc_Code_Similarity/wandb/run-20231010_151817-vv8stjkr</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/messer/huggingface/runs/vv8stjkr' target=\"_blank\">peach-moon-4</a></strong> to <a href='https://wandb.ai/messer/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/messer/huggingface' target=\"_blank\">https://wandb.ai/messer/huggingface</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/messer/huggingface/runs/vv8stjkr' target=\"_blank\">https://wandb.ai/messer/huggingface/runs/vv8stjkr</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2/32 : < :, Epoch 0.06/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 14.29 GB, other allocations: 3.93 GB, max allowed: 18.13 GB). Tried to allocate 24.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 27\u001B[0m\n\u001B[1;32m      3\u001B[0m training_arguments \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[1;32m      4\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mm-messer/JavaDoc_Code_Relevance_Classifier\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      5\u001B[0m     learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2e-5\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     14\u001B[0m     report_to\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwandb\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     15\u001B[0m )\n\u001B[1;32m     17\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m     18\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m     19\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_arguments,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     24\u001B[0m     compute_metrics\u001B[38;5;241m=\u001B[39mcompute_metrics,\n\u001B[1;32m     25\u001B[0m )\n\u001B[0;32m---> 27\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/transformers/trainer.py:1539\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1534\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_wrapped \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\n\u001B[1;32m   1536\u001B[0m inner_training_loop \u001B[38;5;241m=\u001B[39m find_executable_batch_size(\n\u001B[1;32m   1537\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inner_training_loop, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_batch_size, args\u001B[38;5;241m.\u001B[39mauto_find_batch_size\n\u001B[1;32m   1538\u001B[0m )\n\u001B[0;32m-> 1539\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1540\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1541\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1542\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1544\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/transformers/trainer.py:1809\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   1806\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_begin(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[1;32m   1808\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39maccumulate(model):\n\u001B[0;32m-> 1809\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1811\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1812\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   1813\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_tpu_available()\n\u001B[1;32m   1814\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   1815\u001B[0m ):\n\u001B[1;32m   1816\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   1817\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/transformers/trainer.py:2654\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs)\u001B[0m\n\u001B[1;32m   2651\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   2653\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[0;32m-> 2654\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2656\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mn_gpu \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   2657\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mmean()  \u001B[38;5;66;03m# mean() to average on multi-gpu parallel training\u001B[39;00m\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/transformers/trainer.py:2679\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs)\u001B[0m\n\u001B[1;32m   2677\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2678\u001B[0m     labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2679\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2680\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[1;32m   2681\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[1;32m   2682\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:1196\u001B[0m, in \u001B[0;36mRobertaForSequenceClassification.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1189\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[1;32m   1193\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1194\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m-> 1196\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mroberta\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1197\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1200\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1202\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1203\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1204\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1205\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1206\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1207\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1208\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassifier(sequence_output)\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:844\u001B[0m, in \u001B[0;36mRobertaModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    835\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m    837\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[1;32m    838\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m    839\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    842\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[1;32m    843\u001B[0m )\n\u001B[0;32m--> 844\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    845\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    846\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    847\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    848\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    849\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    850\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    851\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    852\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    853\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    854\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    855\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    856\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    857\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:529\u001B[0m, in \u001B[0;36mRobertaEncoder.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    520\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[1;32m    521\u001B[0m         create_custom_forward(layer_module),\n\u001B[1;32m    522\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    526\u001B[0m         encoder_attention_mask,\n\u001B[1;32m    527\u001B[0m     )\n\u001B[1;32m    528\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 529\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    530\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    531\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    532\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    533\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    534\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    535\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    536\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    537\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    539\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    540\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:413\u001B[0m, in \u001B[0;36mRobertaLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    401\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    403\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    410\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m    411\u001B[0m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[1;32m    412\u001B[0m     self_attn_past_key_value \u001B[38;5;241m=\u001B[39m past_key_value[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 413\u001B[0m     self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    414\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    415\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    416\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    418\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    419\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    420\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    422\u001B[0m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:340\u001B[0m, in \u001B[0;36mRobertaAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    331\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    332\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    338\u001B[0m     output_attentions: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    339\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m--> 340\u001B[0m     self_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    341\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    343\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    344\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    345\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    346\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    349\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(self_outputs[\u001B[38;5;241m0\u001B[39m], hidden_states)\n\u001B[1;32m    350\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (attention_output,) \u001B[38;5;241m+\u001B[39m self_outputs[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:278\u001B[0m, in \u001B[0;36mRobertaSelfAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    274\u001B[0m     attention_probs \u001B[38;5;241m=\u001B[39m attention_probs \u001B[38;5;241m*\u001B[39m head_mask\n\u001B[1;32m    276\u001B[0m context_layer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmatmul(attention_probs, value_layer)\n\u001B[0;32m--> 278\u001B[0m context_layer \u001B[38;5;241m=\u001B[39m \u001B[43mcontext_layer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpermute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontiguous\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    279\u001B[0m new_context_layer_shape \u001B[38;5;241m=\u001B[39m context_layer\u001B[38;5;241m.\u001B[39msize()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m+\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mall_head_size,)\n\u001B[1;32m    280\u001B[0m context_layer \u001B[38;5;241m=\u001B[39m context_layer\u001B[38;5;241m.\u001B[39mview(new_context_layer_shape)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: MPS backend out of memory (MPS allocated: 14.29 GB, other allocations: 3.93 GB, max allowed: 18.13 GB). Tried to allocate 24.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(PRETRAINED_MODEL, num_labels=2, id2label=id2label, label2id=label2id)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir='m-messer/JavaDoc_Code_Relevance_Classifier',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=5,\n",
    "    push_to_hub=False,\n",
    "    report_to=[\"wandb\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"valid\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "evaluator = evaluate.evaluator('text-classification')\n",
    "\n",
    "eval_results = evaluator.compute(\n",
    "    model_or_pipeline=model,\n",
    "    data=datasets['test'],\n",
    "    label_mapping=label2id\n",
    ")\n",
    "\n",
    "print(eval_results)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
