{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mmarcus-messer\u001B[0m (\u001B[33mmesser\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /Users/marcus/.netrc\n"
     ]
    }
   ],
   "source": [
    "import torch.cuda\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import wandb\n",
    "import seaborn as sns\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "with open('secrets/wandb_api_key.txt') as f:\n",
    "    wandb.login(key=f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train pre-trained model from hugging face\n",
    "#### Load Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'label', 'text'],\n    num_rows: 315\n})"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.load_from_disk('data/code_search_net_relevance.hf')\n",
    "ds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load tokenizer and preprocess"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# PRETRAINED_MODEL = 'distilbert-base-uncased'\n",
    "PRETRAINED_MODEL = 'microsoft/codebert-base'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/315 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b8551fc1fb1f48c5a365b10b8f0cd08b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'label', 'text', 'input_ids', 'attention_mask'],\n    num_rows: 315\n})"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "\n",
    "def preprocess(data):\n",
    "    return tokenizer(data['text'], truncation=True, padding=True)\n",
    "\n",
    "data_tokens = ds.map(preprocess)\n",
    "data_tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['func_code_string', 'func_documentation_string', 'label', 'input_ids', 'attention_mask'],\n    num_rows: 315\n})"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tokens = data_tokens.remove_columns(['func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens'])\n",
    "data_tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "<Axes: xlabel='label', ylabel='count'>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data_tokens.to_pandas(), x='label')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[75], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_tokens\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3525\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.decode\u001B[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001B[0m\n\u001B[1;32m   3522\u001B[0m \u001B[38;5;66;03m# Convert inputs to python lists\u001B[39;00m\n\u001B[1;32m   3523\u001B[0m token_ids \u001B[38;5;241m=\u001B[39m to_py_obj(token_ids)\n\u001B[0;32m-> 3525\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_decode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3526\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3527\u001B[0m \u001B[43m    \u001B[49m\u001B[43mskip_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3528\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclean_up_tokenization_spaces\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclean_up_tokenization_spaces\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3529\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3530\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:546\u001B[0m, in \u001B[0;36mPreTrainedTokenizerFast._decode\u001B[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001B[0m\n\u001B[1;32m    544\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(token_ids, \u001B[38;5;28mint\u001B[39m):\n\u001B[1;32m    545\u001B[0m     token_ids \u001B[38;5;241m=\u001B[39m [token_ids]\n\u001B[0;32m--> 546\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskip_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_special_tokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    548\u001B[0m clean_up_tokenization_spaces \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    549\u001B[0m     clean_up_tokenization_spaces\n\u001B[1;32m    550\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m clean_up_tokenization_spaces \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    551\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclean_up_tokenization_spaces\n\u001B[1;32m    552\u001B[0m )\n\u001B[1;32m    553\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m clean_up_tokenization_spaces:\n",
      "\u001B[0;31mTypeError\u001B[0m: argument 'ids': 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "tokenizer.decode(data_tokens['input_ids'][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Evaluate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "f1 = evaluate.load('f1')\n",
    "accuracy = evaluate.load('accuracy')\n",
    "recall = evaluate.load('recall')\n",
    "precision = evaluate.load('precision')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy_res = accuracy.compute(predictions=predictions, references=labels)\n",
    "    f1_macro_res = f1.compute(predictions=predictions, references=labels, average='macro')\n",
    "    f1_micro_res = f1.compute(predictions=predictions, references=labels, average='micro')\n",
    "    f1_weighted_res = f1.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    recall_macro_res = recall.compute(predictions=predictions, references=labels, average='macro')\n",
    "    recall_micro_res = recall.compute(predictions=predictions, references=labels, average='micro')\n",
    "    recall_weighted_res = recall.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    precision_macro_res = precision.compute(predictions=predictions, references=labels, average='macro')\n",
    "    precision_micro_res = precision.compute(predictions=predictions, references=labels, average='micro')\n",
    "    precision_weighted_res = precision.compute(predictions=predictions, references=labels, average='weighted')\n",
    "\n",
    "    return {'accuracy': accuracy_res,\n",
    "            'f1_macro': f1_macro_res, 'f1_micro': f1_micro_res, 'f1_weighted': f1_weighted_res,\n",
    "            'recall_macro': recall_macro_res, 'recall_micro': recall_micro_res, 'recall_weighted': recall_weighted_res,\n",
    "            'precision_macro': precision_macro_res, 'precision_micro': precision_micro_res, 'precision_weighted': precision_weighted_res\n",
    "            }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Split train and test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['label', 'text', 'input_ids', 'attention_mask'],\n        num_rows: 252\n    })\n    test: Dataset({\n        features: ['label', 'text', 'input_ids', 'attention_mask'],\n        num_rows: 63\n    })\n})"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_split = data_tokens.train_test_split(test_size=0.2)\n",
    "ds_split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<Axes: xlabel='label', ylabel='count'>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(ds_split['train'].to_pandas(), x='label')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "<Axes: xlabel='label', ylabel='count'>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(ds_split['test'].to_pandas(), x='label')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop('labels')\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 2.0, 2.0], device=model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "id2label = {0: 'irrelevant', 1: 'partially relevant', 2: 'relevant', 3: 'very relevant'}\n",
    "label2id = {'irrelevant': 0, 'partially relevant': 1, 'relevant': 2, 'very relevant': 3}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/marcus/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111819166625436, max=1.0)â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d604d6fcbdde4751ac09b338c94acff2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.12"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/marcus/workspace/Programming/JavaDoc_Code_Similarity/wandb/run-20231010_151817-vv8stjkr</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/messer/huggingface/runs/vv8stjkr' target=\"_blank\">peach-moon-4</a></strong> to <a href='https://wandb.ai/messer/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/messer/huggingface' target=\"_blank\">https://wandb.ai/messer/huggingface</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/messer/huggingface/runs/vv8stjkr' target=\"_blank\">https://wandb.ai/messer/huggingface/runs/vv8stjkr</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2/32 : < :, Epoch 0.06/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 14.29 GB, other allocations: 3.93 GB, max allowed: 18.13 GB). Tried to allocate 24.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 27\u001B[0m\n\u001B[1;32m      3\u001B[0m training_arguments \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[1;32m      4\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mm-messer/JavaDoc_Code_Relevance_Classifier\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      5\u001B[0m     learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2e-5\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     14\u001B[0m     report_to\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwandb\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     15\u001B[0m )\n\u001B[1;32m     17\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m     18\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m     19\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_arguments,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     24\u001B[0m     compute_metrics\u001B[38;5;241m=\u001B[39mcompute_metrics,\n\u001B[1;32m     25\u001B[0m )\n\u001B[0;32m---> 27\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/transformers/trainer.py:1539\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1534\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_wrapped \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\n\u001B[1;32m   1536\u001B[0m inner_training_loop \u001B[38;5;241m=\u001B[39m find_executable_batch_size(\n\u001B[1;32m   1537\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inner_training_loop, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_batch_size, args\u001B[38;5;241m.\u001B[39mauto_find_batch_size\n\u001B[1;32m   1538\u001B[0m )\n\u001B[0;32m-> 1539\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1540\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1541\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1542\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1544\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/transformers/trainer.py:1809\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   1806\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_begin(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[1;32m   1808\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39maccumulate(model):\n\u001B[0;32m-> 1809\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1811\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1812\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   1813\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_tpu_available()\n\u001B[1;32m   1814\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   1815\u001B[0m ):\n\u001B[1;32m   1816\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   1817\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/transformers/trainer.py:2654\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs)\u001B[0m\n\u001B[1;32m   2651\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   2653\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[0;32m-> 2654\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2656\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mn_gpu \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   2657\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mmean()  \u001B[38;5;66;03m# mean() to average on multi-gpu parallel training\u001B[39;00m\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/transformers/trainer.py:2679\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs)\u001B[0m\n\u001B[1;32m   2677\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2678\u001B[0m     labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2679\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2680\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[1;32m   2681\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[1;32m   2682\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:1196\u001B[0m, in \u001B[0;36mRobertaForSequenceClassification.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1189\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[1;32m   1193\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1194\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m-> 1196\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mroberta\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1197\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1200\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1202\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1203\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1204\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1205\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1206\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1207\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1208\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassifier(sequence_output)\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:844\u001B[0m, in \u001B[0;36mRobertaModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    835\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m    837\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[1;32m    838\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m    839\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    842\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[1;32m    843\u001B[0m )\n\u001B[0;32m--> 844\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    845\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    846\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    847\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    848\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    849\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    850\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    851\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    852\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    853\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    854\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    855\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    856\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    857\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:529\u001B[0m, in \u001B[0;36mRobertaEncoder.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    520\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[1;32m    521\u001B[0m         create_custom_forward(layer_module),\n\u001B[1;32m    522\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    526\u001B[0m         encoder_attention_mask,\n\u001B[1;32m    527\u001B[0m     )\n\u001B[1;32m    528\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 529\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    530\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    531\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    532\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    533\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    534\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    535\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    536\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    537\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    539\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    540\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:413\u001B[0m, in \u001B[0;36mRobertaLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    401\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    403\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    410\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m    411\u001B[0m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[1;32m    412\u001B[0m     self_attn_past_key_value \u001B[38;5;241m=\u001B[39m past_key_value[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 413\u001B[0m     self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    414\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    415\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    416\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    418\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    419\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    420\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    422\u001B[0m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:340\u001B[0m, in \u001B[0;36mRobertaAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    331\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    332\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    338\u001B[0m     output_attentions: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    339\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m--> 340\u001B[0m     self_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    341\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    343\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    344\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    345\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    346\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    349\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(self_outputs[\u001B[38;5;241m0\u001B[39m], hidden_states)\n\u001B[1;32m    350\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (attention_output,) \u001B[38;5;241m+\u001B[39m self_outputs[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/workspace/Programming/JavaDoc_Code_Similarity/venv/lib/python3.11/site-packages/transformers/models/roberta/modeling_roberta.py:278\u001B[0m, in \u001B[0;36mRobertaSelfAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    274\u001B[0m     attention_probs \u001B[38;5;241m=\u001B[39m attention_probs \u001B[38;5;241m*\u001B[39m head_mask\n\u001B[1;32m    276\u001B[0m context_layer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmatmul(attention_probs, value_layer)\n\u001B[0;32m--> 278\u001B[0m context_layer \u001B[38;5;241m=\u001B[39m \u001B[43mcontext_layer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpermute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontiguous\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    279\u001B[0m new_context_layer_shape \u001B[38;5;241m=\u001B[39m context_layer\u001B[38;5;241m.\u001B[39msize()[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m+\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mall_head_size,)\n\u001B[1;32m    280\u001B[0m context_layer \u001B[38;5;241m=\u001B[39m context_layer\u001B[38;5;241m.\u001B[39mview(new_context_layer_shape)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: MPS backend out of memory (MPS allocated: 14.29 GB, other allocations: 3.93 GB, max allowed: 18.13 GB). Tried to allocate 24.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(PRETRAINED_MODEL, num_labels=4, id2label=id2label, label2id=label2id)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir='m-messer/JavaDoc_Code_Relevance_Classifier',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=5,\n",
    "    push_to_hub=False,\n",
    "    report_to=[\"wandb\"]\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=ds_split[\"train\"],\n",
    "    eval_dataset=ds_split[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
